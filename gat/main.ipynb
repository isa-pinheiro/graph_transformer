{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "IDf66qdxVcaF"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import ZINC\n",
        "from torch_geometric.loader import DataLoader\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voUgfGGwVdo7",
        "outputId": "b9cbe2fc-0e60-4610-8146-bcd0e601e072"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waRfXZpeVi2L",
        "outputId": "1a7c7c40-8391-4a8d-8d2b-64823efe7f22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tamanho dos datasets:\n",
            "Treino: 10000 amostras\n",
            "Validação: 1000 amostras\n",
            "Teste: 1000 amostras\n"
          ]
        }
      ],
      "source": [
        "train_dataset = ZINC(root='data/ZINC', subset=True, split='train')\n",
        "val_dataset = ZINC(root='data/ZINC', subset=True, split='val')\n",
        "test_dataset = ZINC(root='data/ZINC', subset=True, split='test')\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Tamanho dos datasets:\")\n",
        "print(f\"Treino: {len(train_dataset)} amostras\")\n",
        "print(f\"Validação: {len(val_dataset)} amostras\")\n",
        "print(f\"Teste: {len(test_dataset)} amostras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "8dR2xZDmVlhW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, global_mean_pool\n",
        "from torch import nn\n",
        "\n",
        "class GATGraph(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.heads = 8\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "\n",
        "        # Criando 10 camadas de GATConv\n",
        "        for i in range(10):\n",
        "            in_feats = in_channels if i == 0 else hidden_channels\n",
        "            out_feats = hidden_channels // self.heads if i < 9 else hidden_channels  # Última camada não terá divisão de cabeças\n",
        "            self.convs.append(GATConv(in_feats, out_feats, heads=self.heads, concat=True if i < 9 else False))\n",
        "\n",
        "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.leaky_relu(x)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        return self.lin(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "w48Ad449VnCQ"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
        "        out = out.view(-1)                # [B]\n",
        "        target = batch.y.view(-1).float() # [B]\n",
        "\n",
        "        loss = loss_fn(out, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "bosEhxc6WLka"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def eval_epoch(model, loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch in loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        out = model(batch.x.float(), batch.edge_index, batch.batch)\n",
        "        out = out.view(-1)\n",
        "        target = batch.y.view(-1).float()\n",
        "\n",
        "        loss = loss_fn(out, target)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "WpGkguDKVrY5"
      },
      "outputs": [],
      "source": [
        "def train_gat_regression(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    device,\n",
        "    epochs=50,\n",
        "    lr=1e-3\n",
        "):\n",
        "    model = model.to(device)\n",
        "\n",
        "    loss_fn = torch.nn.L1Loss()  # MAE\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"train_loss\": [],\n",
        "        \"val_mae\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = train_epoch(\n",
        "            model, train_loader, optimizer, loss_fn, device\n",
        "        )\n",
        "        val_mae = eval_epoch(\n",
        "            model, val_loader, loss_fn, device\n",
        "        )\n",
        "\n",
        "        history[\"epoch\"].append(epoch + 1)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_mae\"].append(val_mae)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch+1:03d} | \"\n",
        "            f\"Train Loss: {train_loss:.4f} | \"\n",
        "            f\"Val MAE: {val_mae:.4f}\"\n",
        "        )\n",
        "\n",
        "    test_mae = eval_epoch(model, test_loader, loss_fn, device)\n",
        "    print(f\"Test MAE: {test_mae:.4f}\")\n",
        "\n",
        "    history[\"test_mae\"] = [None] * (epochs - 1) + [test_mae]\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZOb-msPYsdX",
        "outputId": "0d46c3a4-6357-4e4d-f692-94f495148dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.num_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39QwGg6aVuKJ",
        "outputId": "3eaaf342-3944-4da4-be3e-d0d1d83b2130"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | Train Loss: 1.2583 | Val MAE: 1.1233\n",
            "Epoch 002 | Train Loss: 1.1006 | Val MAE: 1.0579\n",
            "Epoch 003 | Train Loss: 1.0483 | Val MAE: 0.9939\n",
            "Epoch 004 | Train Loss: 0.9768 | Val MAE: 0.9182\n",
            "Epoch 005 | Train Loss: 0.9159 | Val MAE: 0.8757\n",
            "Epoch 006 | Train Loss: 0.8808 | Val MAE: 0.8735\n",
            "Epoch 007 | Train Loss: 0.8567 | Val MAE: 0.8360\n",
            "Epoch 008 | Train Loss: 0.8381 | Val MAE: 0.8317\n",
            "Epoch 009 | Train Loss: 0.8172 | Val MAE: 0.8291\n",
            "Epoch 010 | Train Loss: 0.8114 | Val MAE: 0.7990\n",
            "Epoch 011 | Train Loss: 0.8033 | Val MAE: 0.8165\n",
            "Epoch 012 | Train Loss: 0.8058 | Val MAE: 0.8044\n",
            "Epoch 013 | Train Loss: 0.7904 | Val MAE: 0.7877\n",
            "Epoch 014 | Train Loss: 0.7867 | Val MAE: 0.7952\n",
            "Epoch 015 | Train Loss: 0.7796 | Val MAE: 0.7873\n",
            "Epoch 016 | Train Loss: 0.7776 | Val MAE: 0.7742\n",
            "Epoch 017 | Train Loss: 0.7699 | Val MAE: 0.7688\n",
            "Epoch 018 | Train Loss: 0.7710 | Val MAE: 0.7658\n",
            "Epoch 019 | Train Loss: 0.7611 | Val MAE: 0.7598\n",
            "Epoch 020 | Train Loss: 0.7570 | Val MAE: 0.7653\n",
            "Epoch 021 | Train Loss: 0.7618 | Val MAE: 0.7776\n",
            "Epoch 022 | Train Loss: 0.7516 | Val MAE: 0.7633\n",
            "Epoch 023 | Train Loss: 0.7474 | Val MAE: 0.7439\n",
            "Epoch 024 | Train Loss: 0.7452 | Val MAE: 0.7487\n",
            "Epoch 025 | Train Loss: 0.7406 | Val MAE: 0.7428\n",
            "Epoch 026 | Train Loss: 0.7381 | Val MAE: 0.7465\n",
            "Epoch 027 | Train Loss: 0.7365 | Val MAE: 0.7717\n",
            "Epoch 028 | Train Loss: 0.7386 | Val MAE: 0.7626\n",
            "Epoch 029 | Train Loss: 0.7297 | Val MAE: 0.7421\n",
            "Epoch 030 | Train Loss: 0.7255 | Val MAE: 0.7354\n",
            "Epoch 031 | Train Loss: 0.7269 | Val MAE: 0.7411\n",
            "Epoch 032 | Train Loss: 0.7276 | Val MAE: 0.7203\n",
            "Epoch 033 | Train Loss: 0.7146 | Val MAE: 0.7227\n",
            "Epoch 034 | Train Loss: 0.7220 | Val MAE: 0.7225\n",
            "Epoch 035 | Train Loss: 0.7145 | Val MAE: 0.7367\n",
            "Epoch 036 | Train Loss: 0.7073 | Val MAE: 0.7391\n",
            "Epoch 037 | Train Loss: 0.7148 | Val MAE: 0.7190\n",
            "Epoch 038 | Train Loss: 0.7081 | Val MAE: 0.7578\n",
            "Epoch 039 | Train Loss: 0.7067 | Val MAE: 0.7202\n",
            "Epoch 040 | Train Loss: 0.7045 | Val MAE: 0.7081\n",
            "Epoch 041 | Train Loss: 0.7040 | Val MAE: 0.7296\n",
            "Epoch 042 | Train Loss: 0.6923 | Val MAE: 0.7072\n",
            "Epoch 043 | Train Loss: 0.6930 | Val MAE: 0.7081\n",
            "Epoch 044 | Train Loss: 0.6936 | Val MAE: 0.7009\n",
            "Epoch 045 | Train Loss: 0.6960 | Val MAE: 0.7086\n",
            "Epoch 046 | Train Loss: 0.6905 | Val MAE: 0.7321\n",
            "Epoch 047 | Train Loss: 0.6958 | Val MAE: 0.7134\n",
            "Epoch 048 | Train Loss: 0.6890 | Val MAE: 0.7083\n",
            "Epoch 049 | Train Loss: 0.6875 | Val MAE: 0.6860\n",
            "Epoch 050 | Train Loss: 0.6807 | Val MAE: 0.7002\n",
            "Test MAE: 0.7553\n",
            "Epoch 001 | Train Loss: 1.2658 | Val MAE: 1.1620\n",
            "Epoch 002 | Train Loss: 1.1151 | Val MAE: 1.0360\n",
            "Epoch 003 | Train Loss: 0.9936 | Val MAE: 0.9191\n",
            "Epoch 004 | Train Loss: 0.9096 | Val MAE: 0.8628\n",
            "Epoch 005 | Train Loss: 0.8697 | Val MAE: 0.8948\n",
            "Epoch 006 | Train Loss: 0.8334 | Val MAE: 0.8139\n",
            "Epoch 007 | Train Loss: 0.8120 | Val MAE: 0.7999\n",
            "Epoch 008 | Train Loss: 0.7991 | Val MAE: 0.7970\n",
            "Epoch 009 | Train Loss: 0.7954 | Val MAE: 0.7913\n",
            "Epoch 010 | Train Loss: 0.7932 | Val MAE: 0.7889\n",
            "Epoch 011 | Train Loss: 0.7887 | Val MAE: 0.8048\n",
            "Epoch 012 | Train Loss: 0.7832 | Val MAE: 0.7851\n",
            "Epoch 013 | Train Loss: 0.7821 | Val MAE: 0.8037\n",
            "Epoch 014 | Train Loss: 0.7859 | Val MAE: 0.7781\n",
            "Epoch 015 | Train Loss: 0.7796 | Val MAE: 0.7756\n",
            "Epoch 016 | Train Loss: 0.7722 | Val MAE: 0.8036\n",
            "Epoch 017 | Train Loss: 0.7653 | Val MAE: 0.7950\n",
            "Epoch 018 | Train Loss: 0.7607 | Val MAE: 0.7664\n",
            "Epoch 019 | Train Loss: 0.7561 | Val MAE: 0.7636\n",
            "Epoch 020 | Train Loss: 0.7527 | Val MAE: 0.8088\n",
            "Epoch 021 | Train Loss: 0.7567 | Val MAE: 0.7621\n",
            "Epoch 022 | Train Loss: 0.7452 | Val MAE: 0.7645\n",
            "Epoch 023 | Train Loss: 0.7433 | Val MAE: 0.7577\n",
            "Epoch 024 | Train Loss: 0.7392 | Val MAE: 0.7498\n",
            "Epoch 025 | Train Loss: 0.7379 | Val MAE: 0.7772\n",
            "Epoch 026 | Train Loss: 0.7367 | Val MAE: 0.7261\n",
            "Epoch 027 | Train Loss: 0.7360 | Val MAE: 0.7249\n",
            "Epoch 028 | Train Loss: 0.7295 | Val MAE: 0.7452\n",
            "Epoch 029 | Train Loss: 0.7293 | Val MAE: 0.7313\n",
            "Epoch 030 | Train Loss: 0.7300 | Val MAE: 0.7374\n",
            "Epoch 031 | Train Loss: 0.7227 | Val MAE: 0.7256\n",
            "Epoch 032 | Train Loss: 0.7247 | Val MAE: 0.7165\n",
            "Epoch 033 | Train Loss: 0.7193 | Val MAE: 0.7276\n",
            "Epoch 034 | Train Loss: 0.7209 | Val MAE: 0.7270\n",
            "Epoch 035 | Train Loss: 0.7148 | Val MAE: 0.7705\n",
            "Epoch 036 | Train Loss: 0.7163 | Val MAE: 0.7138\n",
            "Epoch 037 | Train Loss: 0.7144 | Val MAE: 0.7242\n",
            "Epoch 038 | Train Loss: 0.7178 | Val MAE: 0.7645\n",
            "Epoch 039 | Train Loss: 0.7174 | Val MAE: 0.7117\n",
            "Epoch 040 | Train Loss: 0.7100 | Val MAE: 0.7034\n",
            "Epoch 041 | Train Loss: 0.7070 | Val MAE: 0.7171\n",
            "Epoch 042 | Train Loss: 0.7086 | Val MAE: 0.7163\n",
            "Epoch 043 | Train Loss: 0.7029 | Val MAE: 0.7226\n",
            "Epoch 044 | Train Loss: 0.7029 | Val MAE: 0.6930\n",
            "Epoch 045 | Train Loss: 0.6999 | Val MAE: 0.7068\n",
            "Epoch 046 | Train Loss: 0.7008 | Val MAE: 0.7251\n",
            "Epoch 047 | Train Loss: 0.6969 | Val MAE: 0.7141\n",
            "Epoch 048 | Train Loss: 0.6963 | Val MAE: 0.6864\n",
            "Epoch 049 | Train Loss: 0.6987 | Val MAE: 0.7032\n",
            "Epoch 050 | Train Loss: 0.6892 | Val MAE: 0.7153\n",
            "Test MAE: 0.7823\n",
            "Epoch 001 | Train Loss: 1.2895 | Val MAE: 1.0791\n",
            "Epoch 002 | Train Loss: 1.0190 | Val MAE: 1.0041\n",
            "Epoch 003 | Train Loss: 0.9056 | Val MAE: 0.9168\n",
            "Epoch 004 | Train Loss: 0.8768 | Val MAE: 0.8510\n",
            "Epoch 005 | Train Loss: 0.8547 | Val MAE: 0.8191\n",
            "Epoch 006 | Train Loss: 0.8259 | Val MAE: 0.8273\n",
            "Epoch 007 | Train Loss: 0.8326 | Val MAE: 0.8176\n",
            "Epoch 008 | Train Loss: 0.8133 | Val MAE: 0.8105\n",
            "Epoch 009 | Train Loss: 0.8014 | Val MAE: 0.7950\n",
            "Epoch 010 | Train Loss: 0.7967 | Val MAE: 0.7839\n",
            "Epoch 011 | Train Loss: 0.7887 | Val MAE: 0.7787\n",
            "Epoch 012 | Train Loss: 0.7741 | Val MAE: 0.7735\n",
            "Epoch 013 | Train Loss: 0.7723 | Val MAE: 0.7837\n",
            "Epoch 014 | Train Loss: 0.7631 | Val MAE: 0.7650\n",
            "Epoch 015 | Train Loss: 0.7617 | Val MAE: 0.7475\n",
            "Epoch 016 | Train Loss: 0.7563 | Val MAE: 0.7697\n",
            "Epoch 017 | Train Loss: 0.7546 | Val MAE: 0.7473\n",
            "Epoch 018 | Train Loss: 0.7528 | Val MAE: 0.7524\n",
            "Epoch 019 | Train Loss: 0.7453 | Val MAE: 0.7356\n",
            "Epoch 020 | Train Loss: 0.7371 | Val MAE: 0.7319\n",
            "Epoch 021 | Train Loss: 0.7340 | Val MAE: 0.7269\n",
            "Epoch 022 | Train Loss: 0.7375 | Val MAE: 0.7343\n",
            "Epoch 023 | Train Loss: 0.7351 | Val MAE: 0.7243\n",
            "Epoch 024 | Train Loss: 0.7347 | Val MAE: 0.7169\n",
            "Epoch 025 | Train Loss: 0.7215 | Val MAE: 0.7251\n",
            "Epoch 026 | Train Loss: 0.7227 | Val MAE: 0.7202\n",
            "Epoch 027 | Train Loss: 0.7250 | Val MAE: 0.7429\n",
            "Epoch 028 | Train Loss: 0.7152 | Val MAE: 0.7151\n",
            "Epoch 029 | Train Loss: 0.7111 | Val MAE: 0.7015\n",
            "Epoch 030 | Train Loss: 0.7095 | Val MAE: 0.7125\n",
            "Epoch 031 | Train Loss: 0.7064 | Val MAE: 0.7214\n",
            "Epoch 032 | Train Loss: 0.7077 | Val MAE: 0.7387\n",
            "Epoch 033 | Train Loss: 0.7056 | Val MAE: 0.7052\n",
            "Epoch 034 | Train Loss: 0.7118 | Val MAE: 0.7278\n",
            "Epoch 035 | Train Loss: 0.7065 | Val MAE: 0.7156\n",
            "Epoch 036 | Train Loss: 0.7043 | Val MAE: 0.7010\n",
            "Epoch 037 | Train Loss: 0.7020 | Val MAE: 0.6947\n",
            "Epoch 038 | Train Loss: 0.7043 | Val MAE: 0.7185\n",
            "Epoch 039 | Train Loss: 0.6972 | Val MAE: 0.6941\n",
            "Epoch 040 | Train Loss: 0.6948 | Val MAE: 0.7077\n",
            "Epoch 041 | Train Loss: 0.6954 | Val MAE: 0.7710\n",
            "Epoch 042 | Train Loss: 0.6967 | Val MAE: 0.7265\n",
            "Epoch 043 | Train Loss: 0.6919 | Val MAE: 0.6991\n",
            "Epoch 044 | Train Loss: 0.6885 | Val MAE: 0.6905\n",
            "Epoch 045 | Train Loss: 0.6882 | Val MAE: 0.6896\n",
            "Epoch 046 | Train Loss: 0.6822 | Val MAE: 0.6831\n",
            "Epoch 047 | Train Loss: 0.6891 | Val MAE: 0.6871\n",
            "Epoch 048 | Train Loss: 0.6876 | Val MAE: 0.7196\n",
            "Epoch 049 | Train Loss: 0.6864 | Val MAE: 0.7304\n",
            "Epoch 050 | Train Loss: 0.6837 | Val MAE: 0.6802\n",
            "Test MAE: 0.7419\n",
            "Epoch 001 | Train Loss: 1.2502 | Val MAE: 1.0852\n",
            "Epoch 002 | Train Loss: 1.0184 | Val MAE: 0.9203\n",
            "Epoch 003 | Train Loss: 0.9154 | Val MAE: 0.9146\n",
            "Epoch 004 | Train Loss: 0.8872 | Val MAE: 0.8652\n",
            "Epoch 005 | Train Loss: 0.8664 | Val MAE: 0.8766\n",
            "Epoch 006 | Train Loss: 0.8357 | Val MAE: 0.8200\n",
            "Epoch 007 | Train Loss: 0.8319 | Val MAE: 0.8519\n",
            "Epoch 008 | Train Loss: 0.8200 | Val MAE: 0.8698\n",
            "Epoch 009 | Train Loss: 0.7994 | Val MAE: 0.7852\n",
            "Epoch 010 | Train Loss: 0.7840 | Val MAE: 0.7741\n",
            "Epoch 011 | Train Loss: 0.7918 | Val MAE: 0.7705\n",
            "Epoch 012 | Train Loss: 0.7743 | Val MAE: 0.7534\n",
            "Epoch 013 | Train Loss: 0.7598 | Val MAE: 0.7677\n",
            "Epoch 014 | Train Loss: 0.7691 | Val MAE: 0.7953\n",
            "Epoch 015 | Train Loss: 0.7584 | Val MAE: 0.7514\n",
            "Epoch 016 | Train Loss: 0.7502 | Val MAE: 0.7348\n",
            "Epoch 017 | Train Loss: 0.7538 | Val MAE: 0.7705\n",
            "Epoch 018 | Train Loss: 0.7435 | Val MAE: 0.7505\n",
            "Epoch 019 | Train Loss: 0.7412 | Val MAE: 0.7293\n",
            "Epoch 020 | Train Loss: 0.7431 | Val MAE: 0.7524\n",
            "Epoch 021 | Train Loss: 0.7329 | Val MAE: 0.7588\n",
            "Epoch 022 | Train Loss: 0.7345 | Val MAE: 0.7419\n",
            "Epoch 023 | Train Loss: 0.7311 | Val MAE: 0.7560\n",
            "Epoch 024 | Train Loss: 0.7271 | Val MAE: 0.7367\n",
            "Epoch 025 | Train Loss: 0.7255 | Val MAE: 0.7348\n",
            "Epoch 026 | Train Loss: 0.7257 | Val MAE: 0.7340\n",
            "Epoch 027 | Train Loss: 0.7251 | Val MAE: 0.7459\n",
            "Epoch 028 | Train Loss: 0.7234 | Val MAE: 0.7249\n",
            "Epoch 029 | Train Loss: 0.7189 | Val MAE: 0.7060\n",
            "Epoch 030 | Train Loss: 0.7145 | Val MAE: 0.7404\n",
            "Epoch 031 | Train Loss: 0.7211 | Val MAE: 0.7485\n",
            "Epoch 032 | Train Loss: 0.7176 | Val MAE: 0.7244\n",
            "Epoch 033 | Train Loss: 0.7166 | Val MAE: 0.7120\n",
            "Epoch 034 | Train Loss: 0.7114 | Val MAE: 0.7108\n",
            "Epoch 035 | Train Loss: 0.7072 | Val MAE: 0.7057\n",
            "Epoch 036 | Train Loss: 0.7094 | Val MAE: 0.6925\n",
            "Epoch 037 | Train Loss: 0.7046 | Val MAE: 0.7317\n",
            "Epoch 038 | Train Loss: 0.7036 | Val MAE: 0.7136\n",
            "Epoch 039 | Train Loss: 0.7039 | Val MAE: 0.7964\n",
            "Epoch 040 | Train Loss: 0.7086 | Val MAE: 0.7250\n",
            "Epoch 041 | Train Loss: 0.7093 | Val MAE: 0.6961\n",
            "Epoch 042 | Train Loss: 0.7015 | Val MAE: 0.7005\n",
            "Epoch 043 | Train Loss: 0.7053 | Val MAE: 0.7134\n",
            "Epoch 044 | Train Loss: 0.7010 | Val MAE: 0.7171\n",
            "Epoch 045 | Train Loss: 0.7012 | Val MAE: 0.7193\n",
            "Epoch 046 | Train Loss: 0.6988 | Val MAE: 0.6867\n",
            "Epoch 047 | Train Loss: 0.6966 | Val MAE: 0.7139\n",
            "Epoch 048 | Train Loss: 0.6961 | Val MAE: 0.7038\n",
            "Epoch 049 | Train Loss: 0.6930 | Val MAE: 0.6871\n",
            "Epoch 050 | Train Loss: 0.6918 | Val MAE: 0.7042\n",
            "Test MAE: 0.7621\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "for seed in [42, 7, 5, 9]:\n",
        "  set_seed(seed)\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model = GATGraph(\n",
        "    in_channels=train_dataset.num_features,\n",
        "    hidden_channels=16,\n",
        "    out_channels=1\n",
        "  )\n",
        "\n",
        "  history = train_gat_regression(\n",
        "      model=model,\n",
        "      train_loader=train_loader,\n",
        "      val_loader=val_loader,\n",
        "      test_loader=test_loader,\n",
        "      device=device,\n",
        "      epochs=50,\n",
        "      lr=1e-3\n",
        "  )\n",
        "\n",
        "  df = pd.DataFrame(history)\n",
        "  df.to_csv(f\"training_metrics.csv_{seed}\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "RDgaiC2yZWlN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
